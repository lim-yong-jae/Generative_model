# GAN
GAN  
CGAN  
infoGAN  
DCGAN  


# Infomation Theory
distance function should staisfy **d(a, b) = d(b, a)**. It is called symmetric

## kl divergence
It dose not satisfy symmetric. So KL(Q||P) != KL(P||Q)
Kl divergence can be used to judge that probability distribution Q and P' shape is close or not.

## kl divergence and MLE
They have some same point
1) MLE method is to calculate parameter that makes probability distribution P, which is used to sample set X, maximize probabiltiy for sampling set X. So it can be written

 



## JS divergence

# loss function

# Problem

# How to solve

# 
